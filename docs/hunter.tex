\section{Hunter: Candidate Generation}
1CLICK is evaluated based on the iUnits in the return text, so the
main task is to detect the relevant iUnits. However, it is difficult
to determine the relevance between queries and iUnits directly. Thus
we consider the relation between iUnits and queries in the context
of passages. In particular, we need to identify candidate iUnits
from query relevant passages, \footnote{In our work passages are
simply defined as overlapped fixed windows in documents, since it
shows that retrieval on fixed length passages can usually perform
better than retrieval on semantic units such as paragraphs
\cite{Callan_SIGIR94,Zobel_etal_IPM95}. We empirically set the
passage length as 120 terms, and overlapping length as 50 terms in
this work.} and and we also need to use passages that contain both
the query and the candidate iUnit as evidence to estimate the
reliability of each candidate iUnit. We took the passages as they
were returned by the search component, i.e., no attempt was made to
further qualify them as answer-bearing passages
\cite{conf/cikm/KrikonCK12} or their perceived relevance
\cite{he-duboue-nie:2012:PAPERS}.

In the component of candidate generation, we first identify possible relevant iUnits as candidates for a specific query.

% primary search
Generally, relevant iUnits should appear in relevant passages, so we need to acquire relevant passages by main search.
In main search, we first identify the phrases in the original query.
Specifically, we use NTLK named entity recognizer to identify the named entities in the query (in the terminology of \cite{conf/aaai/Chu-CarrollF11}, we employ only ``document search'' candidates).
If a phrase is a named entity, we require the words to appear in the exact order in the passages;
The rebuilt query is represented in Indri query language \cite{strohman_ICIA2005}, each token and phrases are connected by the ``combine'' operator.
The details are presented in Table~\ref{table:query}.
For example, for the query ``Whitney Houston death'', we can get the token ``death'' and named entity ``Whitney Houston'' after parsing the query, so the Indri query can be expressed as

\begin{center}
\#combine[passage120:50](\#1(Whitney Houston) death)
\end{center}

\noindent where we set retrieved passage length as 120 and overlap between candidate passages as 50. The ``\#1'' operator means that ``Whitney'' and ``Houston'' should appear together in order and ``\#combine'' combines the matching score of the two components.

\begin{table}
\caption{Indri Query Setting}
\label{table:query}

\centering
\begin{tabular}{|l|l|l|}
\hline
\bf{Text} & \bf{Query} & \bf{Phrase Type} \\
\hline
\hline
A B & \#1(A B) & named entity \\
\hline
A B & \#combine(0.5 \#1(A B)  & pattern phrase \\
    & \ \ \ \ \ \ 0.5 \#combine(A B))     &  \\
\hline

\end{tabular}

\end{table}



% candidate generation
The top passages of ``main search''  are likely to be relevant to the query, and we detect candidate iUnits from a relevant passage pool containing top K passages.
In this stage, we pursue high recall of iUnits, so a large number of candidate iUnits are kept.
We expect most of the irrelevant iUnits can be filtered by the candidate scoring component.
We treat tokens and named entities appearing in the passage pool as candidate iUnits.


% Wikipedia IE
We also consider to extract some ``key information'' from the retrieval passages as candidate iUnits.
For example, for some queries about persons or locations, their properties such as career and birthday can be considered as ``key information''.
To extract such information, we need a training data containing labels about these information, and learn an extractor based on the training data.
In our work, we simply consider the infobox in Wikipedia pages as ``key information''. The infobox in an wikipedia article is a fixed-format table designed to be added at the top right corner of the article, presenting a summary of some aspect the article share. For example, for a wikipedia article about a city, the corresponding infobox usually contains the information about its area, population, time zone, etc. 
In this way, we can get the training data by matching the infobox property text with the corresponding Wikipedia article.
We use mallet \cite{mccallum_02} to train a CRF model \cite{Lafferty_etal_ICML01} for the extraction purpose.
